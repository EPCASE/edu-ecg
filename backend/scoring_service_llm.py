"""
üéØ Service de Scoring S√©mantique - Two-Stage Architecture
Phase 1: LLM extraction (texte ‚Üí IDs ontologie)
Phase 2: Scoring d√©terministe (relations ontologiques)

Auteur: Edu-ECG Team
Date: 2026-01-14
Version: 3.0 (Two-stage: LLM extraction + Ontology scoring)

BACKWARD COMPATIBLE: Garde l'interface SemanticScorer pour correction_llm.py
"""

from typing import List, Dict, Optional
from dataclasses import dataclass
from enum import Enum
import os
import logging
import json

logger = logging.getLogger(__name__)

# Import Two-Stage Architecture
try:
    from backend.scoring_service_two_stage import TwoStageScorer as _TwoStageScorer
    from backend.scoring_service_two_stage import ScoringResult as _TwoStageScoringResult
    TWO_STAGE_AVAILABLE = True
    logger.info("‚úÖ Two-Stage Architecture loaded (LLM extraction + Ontology scoring)")
except Exception as e:
    TWO_STAGE_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Two-Stage unavailable, using legacy: {e}")

# Legacy imports (fallback si two-stage pas dispo)
if not TWO_STAGE_AVAILABLE:
    from openai import OpenAI
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Import ontology services
    try:
        from backend.ontology_service import OntologyService
        _ontology = OntologyService()
        logger.info("‚úÖ Ontology service loaded (legacy)")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Ontology service unavailable: {e}")
        _ontology = None
    
    # Import OWL relation resolver
    try:
        from backend.services.ontology_relations import get_resolver
        _owl_resolver = get_resolver()
        logger.info("‚úÖ OWL Relation Resolver loaded (legacy)")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è OWL Resolver unavailable: {e}")
        _owl_resolver = None


class MatchType(Enum):
    """Types de correspondance entre concepts"""
    EXACT = "exact"              # Correspondance parfaite
    PARENT = "parent"            # √âtudiant trop g√©n√©ral
    CHILD = "child"              # √âtudiant trop sp√©cifique (ou implication valid√©e)
    PARTIAL = "partial"          # Signe correct mais diagnostic incomplet
    SIBLING = "sibling"          # Concepts fr√®res
    CONTRADICTION = "contradiction"  # Concepts contradictoires
    MISSING = "missing"          # Concept attendu manquant
    EXTRA = "extra"              # Concept non attendu


@dataclass
class ConceptMatch:
    """R√©sultat de correspondance d'un concept"""
    student_concept: Optional[str]
    expected_concept: Optional[str]
    match_type: MatchType
    score: float
    explanation: str
    category: str


@dataclass
class ScoringResult:
    """R√©sultat complet du scoring"""
    total_score: float
    max_score: float
    percentage: float
    matches: List[ConceptMatch]
    exact_matches: int
    partial_matches: int
    missing_concepts: int
    extra_concepts: int
    contradictions: int
    category_scores: Dict[str, float]


class SemanticScorer:
    """
    Scoring s√©mantique - NOW WITH TWO-STAGE ARCHITECTURE!
    
    Phase 1: LLM extrait concepts du texte ‚Üí IDs ontologie
    Phase 2: Scoring d√©terministe sur relations ontologiques
    
    BACKWARD COMPATIBLE: Garde l'ancienne interface pour correction_llm.py
    """
    
    def __init__(self):
        """Initialize scorer avec two-stage architecture si disponible."""
        self.category_weights = {
            'rhythm': 1.2,
            'conduction': 1.1,
            'pathology': 1.0,
            'morphology': 0.9,
            'measurement': 0.8
        }
        
        # Utiliser Two-Stage si disponible
        if TWO_STAGE_AVAILABLE:
            self._scorer = _TwoStageScorer(extractor_type="gpt")
            self._mode = "two-stage"
            logger.info("‚úÖ SemanticScorer using TWO-STAGE architecture")
        else:
            self._scorer = None
            self._mode = "legacy"
            logger.warning("‚ö†Ô∏è SemanticScorer using LEGACY mode (two-stage unavailable)")
    
    def score(
        self,
        student_concepts: List[Dict],
        expected_concepts: List[Dict],
        annotations: Optional[List[Dict]] = None,
        territory_selections: Optional[Dict] = None
    ) -> ScoringResult:
        """Score la r√©ponse de l'√©tudiant.
        
        TWO-STAGE MODE (pr√©f√©r√©):
        1. LLM extrait concepts ‚Üí IDs ontologie
        2. Matching ontologique d√©terministe
        
        LEGACY MODE (fallback):
        3. LLM compare chaque paire de concepts
        
        Args:
            student_concepts: Concepts extraits de la r√©ponse √©tudiant
            expected_concepts: Concepts attendus
            annotations: Annotations avec r√¥les (validant/description/exclusion)
            territory_selections: Territoires s√©lectionn√©s
        """
        
        # ===== TWO-STAGE MODE =====
        if self._mode == "two-stage" and self._scorer:
            try:
                result = self._scorer.score(
                    student_concepts,
                    expected_concepts,
                    annotations=annotations,
                    territory_selections=territory_selections
                )
                logger.debug(f"‚úÖ Two-stage scoring: {result.percentage:.0f}% ({result.total_tokens} tokens)")
                return result
            except Exception as e:
                logger.error(f"‚ùå Two-stage failed: {e}, falling back to legacy")
                # Continue vers legacy mode en cas d'erreur
        
        # ===== LEGACY MODE (inchang√© pour compatibilit√©) =====
        logger.info("Using LEGACY scoring mode")
        
        student_normalized = self._normalize_concepts(student_concepts)
        expected_normalized = self._normalize_concepts(expected_concepts)
        
        # üéØ CAS SP√âCIAL: "ECG normal" valide TOUS les concepts normaux
        if self._is_global_normal_statement(student_normalized):
            return self._score_global_normal(expected_normalized)
        
        matches = []
        matched_expected = set()
        
        # 1. Matcher chaque concept √©tudiant
        for student_concept in student_normalized:
            match = self._find_best_match(
                student_concept,
                expected_normalized,
                matched_expected
            )
            matches.append(match)
            
            if match.expected_concept and match.score > 50:
                matched_expected.add(match.expected_concept)
        
        # 2. Concepts manquants - V√âRIFIER si impliqu√©s par un diagnostic donn√©
        for expected_concept in expected_normalized:
            if expected_concept['text'] not in matched_expected:
                # V√©rifier si un concept √©tudiant IMPLIQUE ce concept manquant
                implied_by = self._check_if_implied(student_normalized, expected_concept['text'])
                
                if implied_by:
                    # Concept valid√© par implication
                    matches.append(ConceptMatch(
                        student_concept=implied_by,
                        expected_concept=expected_concept['text'],
                        match_type=MatchType.CHILD,
                        score=100.0,
                        explanation=f"‚úÖ Valid√© par implication: '{implied_by}' implique '{expected_concept['text']}'",
                        category=expected_concept['category']
                    ))
                else:
                    # Vraiment manquant
                    matches.append(ConceptMatch(
                        student_concept=None,
                        expected_concept=expected_concept['text'],
                        match_type=MatchType.MISSING,
                        score=0.0,
                        explanation=f"Concept manquant: {expected_concept['text']}",
                        category=expected_concept['category']
                    ))
        
        # 3. Calculer score total avec p√©nalit√©s territoire
        return self._calculate_total_score(
            matches, 
            len(expected_normalized),
            annotations=annotations,
            territory_selections=territory_selections
        )
    
    def _normalize_concepts(self, concepts: List[Dict]) -> List[Dict]:
        """Normalise les concepts (lowercase, trim)"""
        return [{
            'text': c['text'].lower().strip(),
            'original_text': c['text'],
            'category': c.get('category', 'unknown'),
            'confidence': c.get('confidence', 1.0)
        } for c in concepts]
    
    def _is_global_normal_statement(self, student_concepts: List[Dict]) -> bool:
        """
        D√©tecte si l'√©tudiant a donn√© une r√©ponse globale "ECG normal"
        
        Patterns reconnus:
        - "ecg normal" (comme concept unique)
        - "ecg strictement normal"
        - "trac√© normal"
        - "aucune anomalie"
        - "pas d'anomalie"
        
        NE PAS CONFONDRE avec des concepts sp√©cifiques comme "onde P normale", "QRS normal", etc.
        """
        if not student_concepts:
            return False
        
        # 1. Si un seul concept avec cat√©gorie "global" et texte contient "normal"
        if len(student_concepts) == 1:
            concept = student_concepts[0]
            if concept.get('category') == 'global' and 'normal' in concept['text']:
                logger.info(f"üéØ D√©tection 'ECG normal' global (concept unique): '{concept['text']}'")
                return True
        
        # 2. Si plusieurs concepts, chercher pattern strict "ECG normal" (pas juste "normal")
        full_text = " ".join(c['text'] for c in student_concepts)
        
        global_normal_patterns = [
            'ecg normal',
            'ecg strictement normal',
            'trac√© normal',
            'trac√© strictement normal',
            '√©lectrocardiogramme normal',
            'aucune anomalie',
            "pas d'anomalie",
            'sans anomalie',
            'tout est normal',
            'rythme sinusal normale'  # Cas o√π l'√©tudiant dit juste "rythme sinusal normal" sans d√©tails
        ]
        
        # Patterns SP√âCIFIQUES √† EXCLURE (ne sont PAS des ECG globaux normaux)
        specific_patterns = [
            'onde p normale',
            'onde t normale',
            'qrs normal',
            'pr normal',
            'qt normal',
            'axe normal',
            'repolarisation normale',
            'fr√©quence normale',
            'fr√©quence cardiaque normale'
        ]
        
        # Si on trouve un pattern sp√©cifique, ce n'est PAS un ECG global normal
        for specific in specific_patterns:
            if specific in full_text:
                return False
        
        # Sinon, chercher les patterns globaux
        for pattern in global_normal_patterns:
            if pattern in full_text:
                logger.info(f"üéØ D√©tection 'ECG normal' global: '{full_text}'")
                return True
        
        return False
    
    def _score_global_normal(self, expected_concepts: List[Dict]) -> ScoringResult:
        """
        Score une r√©ponse "ECG normal" globale
        
        Logique:
        - Si TOUS les concepts attendus sont "normaux" ‚Üí 100%
        - Si certains concepts sont pathologiques ‚Üí score partiel avec explication
        """
        matches = []
        
        for expected in expected_concepts:
            # V√©rifier si le concept attendu est "normal"
            is_normal_concept = self._is_normal_concept(expected['text'])
            
            if is_normal_concept:
                # Concept normal ‚Üí valid√© par "ECG normal"
                matches.append(ConceptMatch(
                    student_concept="ecg normal",
                    expected_concept=expected['text'],
                    match_type=MatchType.PARENT,
                    score=100.0,
                    explanation=f"‚úÖ Valid√© par 'ECG normal' (concept parent)",
                    category=expected['category']
                ))
            else:
                # Concept pathologique ‚Üí manquant (l'√©tudiant aurait d√ª le pr√©ciser)
                matches.append(ConceptMatch(
                    student_concept="ecg normal",
                    expected_concept=expected['text'],
                    match_type=MatchType.CONTRADICTION,
                    score=0.0,
                    explanation=f"‚ùå Contradiction: 'ECG normal' mais '{expected['text']}' attendu",
                    category=expected['category']
                ))
        
        return self._calculate_total_score(matches, len(expected_concepts))
    
    def _is_normal_concept(self, concept_text: str) -> bool:
        """
        D√©termine si un concept repr√©sente quelque chose de "normal"
        
        Exemples normaux:
        - "rythme sinusal"
        - "qrs normal", "qrs fins"
        - "fr√©quence normale"
        - "pr normal"
        - "repolarisation normale"
        
        Exemples pathologiques:
        - "bav", "fibrillation"
        - "qrs larges"
        - "tachycardie", "bradycardie"
        - "sus-d√©calage st"
        """
        concept_lower = concept_text.lower()
        
        # Mots-cl√©s normaux
        normal_keywords = [
            'normal', 'sinusal', 'fin', 'fins', '√©troit', 'r√©gulier',
            'r√©guli√®re', 'sans anomalie', 'pas d', 'aucun', 'aucune'
        ]
        
        # Mots-cl√©s pathologiques (prioritaires)
        pathology_keywords = [
            'bav', 'bloc', 'allong√©', 'court', 'large', 'larges',
            'fibrillation', 'flutter', 'tachycardie', 'bradycardie',
            'sus-d√©calage', 'sous-d√©calage', 'infarctus', 'stemi',
            'isch√©mie', 'hypertrophie', 'd√©viation', 'pathologique'
        ]
        
        # V√©rifier d'abord les pathologies (priorit√©)
        for keyword in pathology_keywords:
            if keyword in concept_lower:
                return False
        
        # Puis v√©rifier les mots normaux
        for keyword in normal_keywords:
            if keyword in concept_lower:
                return True
        
        # Par d√©faut: consid√©rer comme normal si pas de pathologie d√©tect√©e
        return True
    
    def _check_medical_implication(self, student_concept: str, expected_concept: str) -> bool:
        """
        V√©rifie si le concept de l'√©tudiant IMPLIQUE le concept attendu
        bas√© sur les relations de l'ontologie m√©dicale OWL
        
        Examples:
            - "BAV 1er degr√©" IMPLIQUE "PR allong√©"
            - "Bloc de branche gauche complet" IMPLIQUE "QRS larges"
            - "Fibrillation auriculaire" IMPLIQUE "Absence d'onde P"
        
        Returns:
            True si le diagnostic de l'√©tudiant implique le finding attendu
        """
        if not _owl_resolver:
            logger.warning("‚ö†Ô∏è OWL Resolver not available, implication check skipped")
            return False
        
        # Utiliser le resolver OWL au lieu du dictionnaire hardcod√©
        is_implied = _owl_resolver.concept_implies(student_concept, expected_concept)
        
        if is_implied:
            logger.info(f"üéØ Implication OWL d√©tect√©e: '{student_concept}' ‚Üí '{expected_concept}'")
        
        return is_implied
    
    def _check_if_implied(self, student_concepts: List[Dict], expected_finding: str) -> Optional[str]:
        """
        V√©rifie si un finding attendu est IMPLIQU√â par un diagnostic donn√© par l'√©tudiant
        
        Example:
            expected_finding = "PR allong√©"
            student_concepts = ["BAV 1er degr√©", ...]
            ‚Üí Returns "bav 1er degr√©" car BAV1 implique PR allong√©
        
        Args:
            student_concepts: Tous les concepts donn√©s par l'√©tudiant
            expected_finding: Le finding attendu (potentiellement manquant)
        
        Returns:
            Le concept √©tudiant qui implique le finding, ou None
        """
        for student_concept in student_concepts:
            student_text = student_concept['text']
            
            # V√©rifier implication m√©dicale
            if self._check_medical_implication(student_text, expected_finding):
                logger.info(f"üéØ Finding '{expected_finding}' impliqu√© par '{student_text}'")
                return student_text
        
        return None
    
    def _find_best_match(
        self,
        student_concept: Dict,
        expected_concepts: List[Dict],
        already_matched: set
    ) -> ConceptMatch:
        """Trouve la meilleure correspondance pour un concept √©tudiant"""
        
        best_match = None
        best_score = -100
        
        for expected in expected_concepts:
            if expected['text'] in already_matched:
                continue
            
            match = self._compare_concepts_llm(student_concept, expected)
            
            if match.score > best_score:
                best_score = match.score
                best_match = match
        
        # Si aucun match, c'est un concept extra
        if best_match is None or best_score < 30:
            return ConceptMatch(
                student_concept=student_concept['text'],
                expected_concept=None,
                match_type=MatchType.EXTRA,
                score=0.0,
                explanation="Concept non attendu (peut √™tre correct)",
                category=student_concept['category']
            )
        
        return best_match
    
    def _compare_concepts_llm(
        self,
        student_concept: Dict,
        expected_concept: Dict
    ) -> ConceptMatch:
        """
        Compare deux concepts avec GPT-4o pour matching s√©mantique
        
        Le LLM comprend:
        - Synonymes m√©dicaux
        - Relations hi√©rarchiques
        - Variations d'expression
        
        ENRICHI avec ontologie m√©dicale:
        - BAV 1 IMPLIQUE PR allong√©
        - BBG IMPLIQUE QRS larges
        """
        student_text = student_concept['text']
        expected_text = expected_concept['text']
        category = expected_concept['category']
        
        # 1. Exact match textuel (rapide)
        if student_text == expected_text:
            return ConceptMatch(
                student_concept=student_text,
                expected_concept=expected_text,
                match_type=MatchType.EXACT,
                score=100.0,
                explanation="‚úÖ Parfait ! Concept exact",
                category=category
            )
        
        # üÜï 1b. Match par inclusion (√©tudiant a donn√© PLUS de d√©tails)
        # Ex: Attendu "p√©ricardite", √âtudiant "p√©ricardite sus-d√©calage inf√©rieur"
        # ‚Üí L'√©tudiant a donn√© le diagnostic + localisation/pr√©cision
        if expected_text in student_text:
            # L'√©tudiant a donn√© le concept attendu + des d√©tails suppl√©mentaires
            return ConceptMatch(
                student_concept=student_text,
                expected_concept=expected_text,
                match_type=MatchType.EXACT,
                score=100.0,
                explanation=f"‚úÖ Parfait ! Concept identifi√© avec pr√©cisions suppl√©mentaires",
                category=category
            )
        
        # 1c. Match partiel inverse (√©tudiant a donn√© concept plus g√©n√©ral)
        # Ex: Attendu "STEMI ant√©rieur", √âtudiant "STEMI"
        if student_text in expected_text:
            # L'√©tudiant a donn√© le concept de base mais sans les d√©tails attendus
            return ConceptMatch(
                student_concept=student_text,
                expected_concept=expected_text,
                match_type=MatchType.PARTIAL,
                score=70.0,
                explanation=f"‚ö†Ô∏è Concept correct mais manque de pr√©cision: '{student_text}' identifi√©, mais attendu '{expected_text}'",
                category=category
            )
        
        # 2. V√©rifier implications m√©dicales (bas√©es sur ontologie)
        # 2a. √âtudiant ‚Üí Attendu (ex: "BAV 1" implique "PR allong√©")
        if self._check_medical_implication(student_text, expected_text):
            return ConceptMatch(
                student_concept=student_text,
                expected_concept=expected_text,
                match_type=MatchType.CHILD,
                score=100.0,
                explanation=f"‚úÖ Valid√© par implication m√©dicale: '{student_text}' implique '{expected_text}'",
                category=category
            )
        
        # 2b. Attendu ‚Üí √âtudiant (ex: √©tudiant dit "PR allong√©" pour "BAV 1")
        # L'√©tudiant a donn√© un SIGNE au lieu du DIAGNOSTIC complet
        if self._check_medical_implication(expected_text, student_text):
            return ConceptMatch(
                student_concept=student_text,
                expected_concept=expected_text,
                match_type=MatchType.PARTIAL,
                score=40.0,  # Score partiel
                explanation=f"‚ö†Ô∏è Signe correct mais incomplet: '{student_text}' est un signe de '{expected_text}', mais pas le diagnostic complet",
                category=category
            )
        
        # 3. Matching s√©mantique avec GPT-4o-mini
        try:
            prompt = f"""Tu es un expert cardiologue. Compare ces deux concepts ECG:

Concept √©tudiant: "{student_text}"
Concept attendu: "{expected_text}"

D√©termine leur relation s√©mantique M√âDICALE:

- EQUIVALENT: Synonymes ou √©quivalents
  * "BAV 1" = "BAV de type 1" = "BAV du 1er degr√©"
  * "QRS fins" = "QRS normaux"
  * "P√©ricardite" = "P√©ricardite sus-d√©calage" (m√™me diagnostic avec pr√©cision suppl√©mentaire)
  * Si le concept √©tudiant CONTIENT le concept attendu + d√©tails ‚Üí EQUIVALENT

- CHILD: L'√©tudiant a donn√© un DIAGNOSTIC qui implique le SIGNE attendu
  * √âtudiant dit "BAV 1" pour "PR allong√©" attendu
  * √âtudiant dit "BBG complet" pour "QRS larges" attendu
  * Le diagnostic EXPLIQUE le signe

- PARENT: L'√©tudiant a donn√© SEULEMENT un SIGNE pour un DIAGNOSTIC attendu
  * √âtudiant dit "PR allong√©" pour "BAV 1" attendu (manque le diagnostic)
  * √âtudiant dit "onde P bloqu√©e" pour "BAV 2" attendu (manque le diagnostic)
  * ‚ö†Ô∏è NE PAS confondre avec "diagnostic + signe" qui est EQUIVALENT

- SIBLING: Concepts reli√©s mais diff√©rents (ex: "QRS larges" vs "QRS fins")

- DIFFERENT: Concepts totalement diff√©rents

‚ö†Ô∏è R√àGLES IMPORTANTES:
1. Si l'√©tudiant donne le DIAGNOSTIC attendu + des d√©tails (territoire, localisation) ‚Üí EQUIVALENT, pas PARENT
2. PARENT uniquement si l'√©tudiant donne SEULEMENT un signe SANS le diagnostic
3. "P√©ricardite sus-d√©calage" contient "P√©ricardite" ‚Üí EQUIVALENT

R√©ponds UNIQUEMENT avec ce JSON:
{{"relationship": "EQUIVALENT|CHILD|PARENT|SIBLING|DIFFERENT", "confidence": 0.0-1.0, "explanation": "courte explication en fran√ßais"}}"""

            response = client.chat.completions.create(
                model="gpt-4o-mini",  # Plus rapide et √©conomique
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=150,
                timeout=5
            )
            
            result = json.loads(response.choices[0].message.content)
            relationship = result['relationship']
            confidence = result['confidence']
            explanation = result['explanation']
            
            logger.info(f"ü§ñ LLM match: '{student_text}' vs '{expected_text}' ‚Üí {relationship} ({confidence:.2f})")
            
            # Convertir relation LLM en score
            if relationship == 'EQUIVALENT' and confidence > 0.7:
                return ConceptMatch(
                    student_concept=student_text,
                    expected_concept=expected_text,
                    match_type=MatchType.EXACT,
                    score=100.0,
                    explanation=f"‚úÖ {explanation}",
                    category=category
                )
            
            elif relationship == 'CHILD' and confidence > 0.6:
                return ConceptMatch(
                    student_concept=student_text,
                    expected_concept=expected_text,
                    match_type=MatchType.CHILD,
                    score=90.0,
                    explanation=f"‚úÖ Tr√®s bien ! {explanation}",
                    category=category
                )
            
            elif relationship == 'PARENT' and confidence > 0.6:
                # PARENT = L'√©tudiant a donn√© un SIGNE pour un DIAGNOSTIC attendu
                # Ex: "onde P bloqu√©e" pour "BAV 2 Mobitz 2"
                # Score partiel car signe correct mais diagnostic incomplet
                return ConceptMatch(
                    student_concept=student_text,
                    expected_concept=expected_text,
                    match_type=MatchType.PARTIAL,
                    score=40.0,  # Score partiel coh√©rent avec requiresFindings inverse
                    explanation=f"‚ö†Ô∏è Signe correct mais diagnostic incomplet : {explanation}",
                    category=category
                )
            
            elif relationship == 'SIBLING' and confidence > 0.6:
                return ConceptMatch(
                    student_concept=student_text,
                    expected_concept=expected_text,
                    match_type=MatchType.SIBLING,
                    score=50.0,
                    explanation=f"‚ö†Ô∏è {explanation}",
                    category=category
                )
            
            else:  # DIFFERENT
                return ConceptMatch(
                    student_concept=student_text,
                    expected_concept=expected_text,
                    match_type=MatchType.MISSING,
                    score=0.0,
                    explanation=f"‚ùå {explanation}",
                    category=category
                )
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LLM matching failed: {e}, using fallback")
            
            # Fallback: similarit√© textuelle basique
            if self._text_similarity(student_text, expected_text) > 0.8:
                return ConceptMatch(
                    student_concept=student_text,
                    expected_concept=expected_text,
                    match_type=MatchType.EXACT,
                    score=95.0,
                    explanation="‚úÖ Tr√®s proche (fallback textuel)",
                    category=category
                )
            else:
                return ConceptMatch(
                    student_concept=student_text,
                    expected_concept=expected_text,
                    match_type=MatchType.MISSING,
                    score=0.0,
                    explanation="‚ùå Diff√©rent (LLM indisponible)",
                    category=category
                )
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """Similarit√© textuelle fallback (Jaccard)"""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _calculate_total_score(
        self,
        matches: List[ConceptMatch],
        num_expected: int,
        annotations: Optional[List[Dict]] = None,
        territory_selections: Optional[Dict] = None
    ) -> ScoringResult:
        """Calcule le score total et les statistiques
        
        Args:
            matches: Liste des correspondances concept √©tudiant/attendu
            num_expected: Nombre de concepts attendus
            annotations: Annotations avec r√¥les (pour p√©nalit√© territoire)
            territory_selections: Territoires s√©lectionn√©s par concept
        """
        
        # üÜï APPLIQUER P√âNALIT√â TERRITOIRE (-50% pour diagnostics validants sans territoire)
        if annotations and territory_selections is not None:
            for match in matches:
                if match.expected_concept and match.score > 0:
                    # Trouver l'annotation correspondante
                    matching_annotation = None
                    for ann in annotations:
                        if ann['concept'] == match.expected_concept:
                            matching_annotation = ann
                            break
                    
                    if matching_annotation:
                        # V√©rifier si c'est un diagnostic validant
                        is_validant = matching_annotation.get('annotation_role', 'üìù Description') == 'üéØ Diagnostic validant'
                        
                        # V√©rifier si le concept n√©cessite un territoire
                        has_territory_possibles = bool(matching_annotation.get('territoires_possibles'))
                        
                        if is_validant and has_territory_possibles:
                            # V√©rifier si un territoire a √©t√© s√©lectionn√©
                            concept_name = match.expected_concept
                            territories = territory_selections.get(concept_name, {}).get('territories', [])
                            
                            if not territories:
                                # P√©nalit√© -50%
                                match.score = match.score * 0.5
                                match.explanation += " ‚ö†Ô∏è Territoire manquant (-50%)"
                                logger.info(f"P√©nalit√© territoire appliqu√©e √† '{concept_name}': {match.score}")
        
        total_score = sum(m.score for m in matches if m.expected_concept)
        max_score = num_expected * 100.0
        percentage = (total_score / max_score * 100) if max_score > 0 else 0
        
        # Statistiques
        exact = sum(1 for m in matches if m.match_type == MatchType.EXACT)
        partial = sum(1 for m in matches if m.match_type in [MatchType.CHILD, MatchType.PARENT, MatchType.SIBLING])
        missing = sum(1 for m in matches if m.match_type == MatchType.MISSING)
        extra = sum(1 for m in matches if m.match_type == MatchType.EXTRA)
        contradictions = sum(1 for m in matches if m.match_type == MatchType.CONTRADICTION)
        
        # Scores par cat√©gorie
        category_scores = {}
        for category in ['rhythm', 'conduction', 'morphology', 'measurement', 'pathology']:
            cat_matches = [m for m in matches if m.category == category and m.expected_concept]
            if cat_matches:
                cat_score = sum(m.score for m in cat_matches) / len(cat_matches)
                category_scores[category] = cat_score
        
        return ScoringResult(
            total_score=total_score,
            max_score=max_score,
            percentage=percentage,
            matches=matches,
            exact_matches=exact,
            partial_matches=partial,
            missing_concepts=missing,
            extra_concepts=extra,
            contradictions=contradictions,
            category_scores=category_scores
        )
