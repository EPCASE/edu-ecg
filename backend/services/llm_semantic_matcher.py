"""
üß† LLM Semantic Matcher - Matching S√©mantique Intelligent
=========================================================

Architecture Hybride :
- LLM = Traducteur s√©mantique (comprendre variations linguistiques)
- Syst√®me = Contr√¥le total du scoring (ontologie, poids, hi√©rarchie)

Date : 2026-01-10
Sprint : 1 - Phase Prototype
"""

import os
import json
import time
from typing import Dict, List, Optional, Tuple
from openai import OpenAI
from dotenv import load_dotenv

# Charger variables d'environnement
load_dotenv()

# Import cache service (Sprint 2)
try:
    from llm_cache_service import (
        get_cached_match,
        set_cached_match,
        get_cache_stats,
        health_check as cache_health_check
    )
    CACHE_AVAILABLE = True
except ImportError:
    try:
        # Fallback pour import depuis racine projet
        from backend.services.llm_cache_service import (
            get_cached_match,
            set_cached_match,
            get_cache_stats,
            health_check as cache_health_check
        )
        CACHE_AVAILABLE = True
    except ImportError:
        CACHE_AVAILABLE = False
        print("‚ö†Ô∏è Cache service non disponible - fonctionnement sans cache")

# ============================================================================
# CONFIGURATION
# ============================================================================

# Initialiser client OpenAI (peut √™tre None si pas de cl√©)
try:
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        client = OpenAI(api_key=api_key)
    else:
        client = None
        print("‚ö†Ô∏è OPENAI_API_KEY non trouv√©e - LLM Semantic Matcher d√©sactiv√©")
except Exception as e:
    client = None
    print(f"‚ö†Ô∏è Erreur initialisation OpenAI client : {e}")

# Types de match possibles
MATCH_TYPE_EXACT = "exact"              # "BAV 2 Mobitz 1" == "BAV 2 Mobitz 1"
MATCH_TYPE_SYNONYM = "synonym"          # "BAV2M1" ~= "BAV 2 Mobitz 1"
MATCH_TYPE_ABBREVIATION = "abbreviation"  # "RS" ~= "Rythme sinusal"
MATCH_TYPE_PARENT = "parent"            # "QRS normal" ~= "QRS fins" (parent)
MATCH_TYPE_CHILD = "child"              # "QRS fins" ~= "QRS normal" (child)
MATCH_TYPE_EQUIVALENT = "equivalent"    # "Sinusal" ~= "Rythme sinusal"
MATCH_TYPE_NO_MATCH = "no_match"        # Aucune correspondance

# Seuil de confiance minimum pour accepter un match (0-100)
CONFIDENCE_THRESHOLD = 70


# ============================================================================
# PROMPT SYST√àME POUR LE LLM
# ============================================================================

SYSTEM_PROMPT = """Tu es un expert en √©lectrocardiographie (ECG) et en terminologie m√©dicale.

**TON R√îLE UNIQUE : MATCHING S√âMANTIQUE**

Tu NE dois PAS :
- Calculer de score
- D√©cider de la validit√© m√©dicale
- Attribuer des points
- √âvaluer la qualit√© d'une r√©ponse

Tu DOIS SEULEMENT :
- D√©terminer si deux concepts ECG sont √©quivalents, synonymes, ou reli√©s
- Identifier le type de relation (exact, synonyme, abr√©viation, parent/enfant)
- Estimer ta confiance dans le match (0-100%)
- Expliquer ta logique de matching

**CONTEXTE M√âDICAL :**
- Les √©tudiants en m√©decine utilisent souvent des abr√©viations (BAV2M1, RS, BBG)
- Les termes peuvent varier (Sinusal vs Rythme sinusal, QRS fin vs QRS normal)
- La hi√©rarchie m√©dicale est importante (QRS normal inclut QRS fins, Axe normal, etc.)
- Les fautes de frappe sont courantes (Sinusl, BAV2 M1 avec espace)

**TYPES DE MATCH √Ä IDENTIFIER :**

1. **exact** : Identiques (casse insensible)
   - Ex: "BAV 2 Mobitz 1" == "bav 2 mobitz 1"

2. **synonym** : Synonymes m√©dicaux
   - Ex: "Wenckebach" ~= "BAV 2 Mobitz 1"

3. **abbreviation** : Abr√©viation standard
   - Ex: "BAV2M1" ~= "BAV 2 Mobitz 1"
   - Ex: "RS" ~= "Rythme sinusal"
   - Ex: "BBG" ~= "Bloc de branche gauche"

4. **equivalent** : Variantes √©quivalentes
   - Ex: "Sinusal" ~= "Rythme sinusal"
   - Ex: "QRS fin" ~= "QRS fins" (singulier/pluriel)

5. **parent** : Concept √©tudiant est parent du concept attendu
   - Ex: √âtudiant dit "QRS normal", attendu "QRS fins"
   - QRS normal (parent) inclut QRS fins (enfant)
   - Match partiel acceptable

6. **child** : Concept √©tudiant est enfant du concept attendu
   - Ex: √âtudiant dit "QRS fins", attendu "QRS normal"
   - Plus pr√©cis qu'attendu (bon signe)

7. **no_match** : Aucune relation
   - Ex: "Bloc de branche gauche" ‚â† "Rythme sinusal"

**FORMAT DE R√âPONSE STRICT :**

R√©ponds UNIQUEMENT en JSON valide :

{
  "match": true/false,
  "match_type": "exact|synonym|abbreviation|equivalent|parent|child|no_match",
  "confidence": 0-100,
  "explanation": "Explication courte du matching"
}

**EXEMPLES :**

Input: student="BAV2M1", expected="BAV 2 Mobitz 1"
Output: {"match": true, "match_type": "abbreviation", "confidence": 95, "explanation": "BAV2M1 est l'abr√©viation standard de BAV 2 Mobitz 1"}

Input: student="Sinusal", expected="Rythme sinusal"
Output: {"match": true, "match_type": "equivalent", "confidence": 90, "explanation": "Sinusal est une variante courante de Rythme sinusal"}

Input: student="QRS normal", expected="QRS fins"
Output: {"match": true, "match_type": "parent", "confidence": 80, "explanation": "QRS normal est le concept parent qui inclut QRS fins"}

Input: student="BBG", expected="Bloc de branche droit"
Output: {"match": false, "match_type": "no_match", "confidence": 100, "explanation": "BBG signifie Bloc de branche gauche, pas droit"}

**RAPPEL : Tu es un traducteur s√©mantique, PAS un correcteur. Le scoring est g√©r√© par l'ontologie syst√®me.**
"""


# ============================================================================
# FONCTION PRINCIPALE : SEMANTIC MATCH
# ============================================================================

def semantic_match(
    student_concept: str,
    expected_concept: str,
    ontology_context: Optional[Dict] = None
) -> Dict:
    """
    D√©termine si un concept √©tudiant correspond s√©mantiquement √† un concept attendu.
    
    ARCHITECTURE SPRINT 2 :
    1. Essayer cache Redis (si disponible)
    2. Si cache miss ‚Üí Appel LLM
    3. Stocker r√©sultat en cache
    
    LE LLM NE CALCULE PAS LE SCORE ! Il fait seulement du matching s√©mantique.
    Le syst√®me utilise ensuite l'ontologie pour scorer selon poids/hi√©rarchie.
    
    Args:
        student_concept: Concept √©crit par l'√©tudiant (ex: "BAV2M1")
        expected_concept: Concept attendu de l'ontologie (ex: "BAV 2 Mobitz 1")
        ontology_context: Contexte ontologique optionnel (hi√©rarchie, synonymes OWL)
    
    Returns:
        {
            "match": bool,                  # Est-ce un match ?
            "match_type": str,              # Type de match (exact, synonym, etc.)
            "confidence": int,              # Confiance 0-100
            "explanation": str,             # Explication du matching
            "student_concept": str,         # Concept √©tudiant (echo)
            "expected_concept": str,        # Concept attendu (echo)
            "ontology_context_used": bool,  # Contexte ontologie utilis√© ?
            "cached": bool,                 # R√©sultat vient du cache ?
            "latency_ms": float             # Temps de r√©ponse en ms
        }
    """
    start_time = time.time()
    
    # ====================================================================
    # PHASE 1 : ESSAYER LE CACHE (Sprint 2)
    # ====================================================================
    
    if CACHE_AVAILABLE:
        try:
            cached_result = get_cached_match(student_concept, expected_concept)
            if cached_result:
                # Cache HIT - retourner imm√©diatement
                latency_ms = (time.time() - start_time) * 1000
                cached_result["latency_ms"] = latency_ms
                return cached_result
        except Exception as e:
            # Cache error - continuer sans cache
            print(f"‚ö†Ô∏è Erreur cache (continuing without): {e}")
    
    # ====================================================================
    # PHASE 2 : APPEL LLM (Cache miss ou cache indisponible)
    # ====================================================================
    
    # Construire le prompt utilisateur
    user_prompt = f"""
D√©termine si ces deux concepts ECG correspondent :

**Concept √©tudiant :** "{student_concept}"
**Concept attendu :** "{expected_concept}"
"""
    
    # Ajouter contexte ontologique si disponible
    if ontology_context:
        user_prompt += f"\n**Contexte ontologique :**\n```json\n{json.dumps(ontology_context, indent=2, ensure_ascii=False)}\n```\n"
    
    user_prompt += "\nR√©ponds en JSON uniquement."
    
    # V√©rifier que le client est disponible
    if not client:
        result = {
            "match": False,
            "match_type": MATCH_TYPE_NO_MATCH,
            "confidence": 0,
            "explanation": "LLM non disponible (OPENAI_API_KEY manquante)",
            "student_concept": student_concept,
            "expected_concept": expected_concept,
            "ontology_context_used": False,
            "cached": False,
            "latency_ms": (time.time() - start_time) * 1000,
            "error": "No OpenAI client"
        }
        return result
    
    try:
        # Appel API OpenAI
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,  # Tr√®s bas pour coh√©rence maximale
            max_tokens=300,
            response_format={"type": "json_object"}  # Force JSON
        )
        
        # Parser la r√©ponse
        result = json.loads(response.choices[0].message.content)
        
        # Validation structure
        if not all(k in result for k in ["match", "match_type", "confidence", "explanation"]):
            raise ValueError("R√©ponse LLM incompl√®te")
        
        # Normalisation confiance (au cas o√π LLM retourne float)
        result["confidence"] = int(result["confidence"])
        
        # Clamp confidence 0-100
        result["confidence"] = max(0, min(100, result["confidence"]))
        
        # Appliquer seuil de confiance
        if result["confidence"] < CONFIDENCE_THRESHOLD:
            result["match"] = False
            result["match_type"] = MATCH_TYPE_NO_MATCH
            result["explanation"] += f" (Confiance {result['confidence']}% < seuil {CONFIDENCE_THRESHOLD}%)"
        
        # Ajouter m√©tadonn√©es
        result["student_concept"] = student_concept
        result["expected_concept"] = expected_concept
        result["ontology_context_used"] = ontology_context is not None
        result["cached"] = False
        result["latency_ms"] = (time.time() - start_time) * 1000
        
        # ====================================================================
        # PHASE 3 : STOCKER EN CACHE (Sprint 2)
        # ====================================================================
        
        if CACHE_AVAILABLE and result["match"]:
            try:
                set_cached_match(student_concept, expected_concept, result)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur stockage cache: {e}")
        
        return result
        
    except Exception as e:
        # Fallback en cas d'erreur API
        latency_ms = (time.time() - start_time) * 1000
        return {
            "match": False,
            "match_type": MATCH_TYPE_NO_MATCH,
            "confidence": 0,
            "explanation": f"Erreur LLM : {str(e)}",
            "student_concept": student_concept,
            "expected_concept": expected_concept,
            "ontology_context_used": False,
            "cached": False,
            "latency_ms": latency_ms,
            "error": str(e)
        }


# ============================================================================
# FONCTION BATCH : MATCHER PLUSIEURS CONCEPTS
# ============================================================================

def batch_semantic_match(
    student_concepts: List[str],
    expected_concepts: List[str],
    ontology: Dict
) -> List[Dict]:
    """
    Matche une liste de concepts √©tudiants contre des concepts attendus.
    
    Args:
        student_concepts: Liste concepts √©tudiants
        expected_concepts: Liste concepts attendus (ontologie)
        ontology: Ontologie compl√®te (pour contexte)
    
    Returns:
        Liste de r√©sultats de matching
    """
    results = []
    
    for student_concept in student_concepts:
        best_match = None
        best_confidence = 0
        
        # Chercher le meilleur match parmi tous les concepts attendus
        for expected_concept in expected_concepts:
            # Extraire contexte ontologique pour ce concept
            context = _extract_ontology_context(expected_concept, ontology)
            
            # Faire le matching
            match_result = semantic_match(student_concept, expected_concept, context)
            
            # Garder le meilleur match
            if match_result["match"] and match_result["confidence"] > best_confidence:
                best_match = match_result
                best_confidence = match_result["confidence"]
        
        # Ajouter r√©sultat (ou no_match si rien trouv√©)
        if best_match:
            results.append(best_match)
        else:
            results.append({
                "match": False,
                "match_type": MATCH_TYPE_NO_MATCH,
                "confidence": 0,
                "explanation": f"Aucun concept attendu ne correspond √† '{student_concept}'",
                "student_concept": student_concept,
                "expected_concept": None,
                "ontology_context_used": False
            })
    
    return results


# ============================================================================
# HELPERS
# ============================================================================

def _extract_ontology_context(concept_name: str, ontology: Dict) -> Dict:
    """
    Extrait le contexte ontologique pertinent pour un concept.
    
    Args:
        concept_name: Nom du concept (ex: "BAV 2 Mobitz 1")
        ontology: Ontologie compl√®te
    
    Returns:
        Contexte pertinent (synonymes, hi√©rarchie, cat√©gorie)
    """
    context = {}
    
    # Chercher le concept dans l'ontologie
    for concept_id, concept_data in ontology.items():
        if concept_data.get("name") == concept_name:
            context = {
                "id": concept_id,
                "name": concept_data.get("name"),
                "synonyms": concept_data.get("synonyms", []),
                "category": concept_data.get("category"),
                "implications": concept_data.get("implications", []),
                "weight": concept_data.get("weight", 1)
            }
            break
    
    return context


def get_match_type_emoji(match_type: str) -> str:
    """Retourne un emoji pour visualiser le type de match."""
    emojis = {
        MATCH_TYPE_EXACT: "üéØ",
        MATCH_TYPE_SYNONYM: "üîÑ",
        MATCH_TYPE_ABBREVIATION: "üìù",
        MATCH_TYPE_EQUIVALENT: "‚âà",
        MATCH_TYPE_PARENT: "‚¨ÜÔ∏è",
        MATCH_TYPE_CHILD: "‚¨áÔ∏è",
        MATCH_TYPE_NO_MATCH: "‚ùå"
    }
    return emojis.get(match_type, "‚ùì")


def get_match_type_label(match_type: str) -> str:
    """Retourne un label fran√ßais pour le type de match."""
    labels = {
        MATCH_TYPE_EXACT: "Correspondance exacte",
        MATCH_TYPE_SYNONYM: "Synonyme m√©dical",
        MATCH_TYPE_ABBREVIATION: "Abr√©viation",
        MATCH_TYPE_EQUIVALENT: "√âquivalent",
        MATCH_TYPE_PARENT: "Concept parent (partiel)",
        MATCH_TYPE_CHILD: "Concept enfant (pr√©cis)",
        MATCH_TYPE_NO_MATCH: "Aucune correspondance"
    }
    return labels.get(match_type, "Type inconnu")


def get_llm_stats() -> Dict:
    """
    Retourne les statistiques LLM + Cache.
    
    Returns:
        Dict avec cache stats + LLM info
    """
    stats = {
        "llm_available": client is not None,
        "cache_available": CACHE_AVAILABLE,
        "model": "gpt-4o",
        "temperature": 0.1,
        "confidence_threshold": CONFIDENCE_THRESHOLD
    }
    
    # Ajouter stats cache si disponible
    if CACHE_AVAILABLE:
        try:
            cache_stats = get_cache_stats()
            stats["cache"] = cache_stats
            
            # Ajouter cache health
            cache_health = cache_health_check()
            stats["cache_health"] = cache_health["status"]
        except Exception as e:
            stats["cache"] = {"error": str(e)}
            stats["cache_health"] = "error"
    
    return stats


# ============================================================================
# MAIN - TESTS
# ============================================================================

if __name__ == "__main__":
    """Tests de validation du matching s√©mantique."""
    
    print("üß† Tests LLM Semantic Matcher\n")
    
    if not client:
        print("‚ùå OpenAI client non disponible - d√©finir OPENAI_API_KEY dans .env")
        print("‚ÑπÔ∏è  Pour tester, cr√©er un fichier .env √† la racine avec:")
        print("   OPENAI_API_KEY=sk-...")
        exit(1)
    
    # Test 1 : Abr√©viation
    print("Test 1 : BAV2M1 vs BAV 2 Mobitz 1")
    result = semantic_match("BAV2M1", "BAV 2 Mobitz 1")
    print(f"  Match: {result['match']} ({result['match_type']}, {result['confidence']}%)")
    print(f"  Explication: {result['explanation']}\n")
    
    # Test 2 : √âquivalent
    print("Test 2 : Sinusal vs Rythme sinusal")
    result = semantic_match("Sinusal", "Rythme sinusal")
    print(f"  Match: {result['match']} ({result['match_type']}, {result['confidence']}%)")
    print(f"  Explication: {result['explanation']}\n")
    
    # Test 3 : Parent
    print("Test 3 : QRS normal vs QRS fins")
    result = semantic_match("QRS normal", "QRS fins")
    print(f"  Match: {result['match']} ({result['match_type']}, {result['confidence']}%)")
    print(f"  Explication: {result['explanation']}\n")
    
    # Test 4 : No match
    print("Test 4 : BBG vs Bloc de branche droit")
    result = semantic_match("BBG", "Bloc de branche droit")
    print(f"  Match: {result['match']} ({result['match_type']}, {result['confidence']}%)")
    print(f"  Explication: {result['explanation']}\n")
    
    print("‚úÖ Tests termin√©s")
